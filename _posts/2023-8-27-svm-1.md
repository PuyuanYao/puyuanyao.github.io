---
title: Support vector machines (1) - the optimal margin classifier
date: 2023-8-27
categories: [classification]
tags: [svm]
math: true
---

# Support vector machines (1) - the optimal margin classifier

Support vector machines (SVM) are designed to solve classification problems. The most simple classification problem is separable case where data can be split by a line, plane, or hyperplane. And the method that solves this problem is the optimal margin classifier.  

To begin with, we have the dataset 
$$\{(x_1, y_1),(x_2,y_2),...,(x_n,y_n)\}\text{, where }x\in R^p\text{, and }y\in\{-1,1\}$$
and hyperplane
$$w^Tx+b$$

## Functional margin
Given a set of parameter $w$ and $b$, we can classify an example $x_i$ by calculating $w^Tx_i+b$. Then we can classify $x_i$ by looking the result is positive or negative. Further more, we define the functional margin of a example as
$$\hat{\gamma}_i=y_i(w^Tx_i+b).$$
We define the functional margin of a dataset as
$$\hat{\gamma}=\min_{i=1,...,n}y_i(w^Tx_i+b).$$
Functional margin represents the correctness and the confidence of a prediction. If $w^Tx_i+b$ and $y_i$ has the same sign, it means that the prediction is correct, and $\gamma_i$ would be positive, vice versa. By confidence, the larger $\gamma_i$ is, the more confidence you have in this prediction.  
However, if we scale up $w$ and $b$ to for example $2w$ and $2b$, then $\hat{\gamma}_i$ will scale up 2 times, but our confidence to this prediction won't be 2 times higher because $w^Tx+b$ and $2w^T+2b$ are the same hyperplane. Therefore, the functional margin only tells you whether the prediction is correct.  

## Geometric margin
![Light mode only](/assets/posts-img/svm-1-light.jpg){: .light }
![Dark mode only](/assets/posts-img/svm-1-dark.jpg){: .dark }  
Given an example $x_i$, and its projection on the hyperplane $\widetilde{x}_i$, the distance from $x_i$ to the hyperplane is called geometrix margin, denoted $\gamma_i$.  
Since $w$ is orthogonal to the hyperplane, 
$$\widetilde{x}_i=x_i-\gamma_i\frac{w}{||w||}.$$
Because $\widetilde{x}_i$ is on the hyperplane, 
$$w^T(\widetilde{x}_i)+b=0$$
$$w^T(x_i-\gamma_i\frac{w}{||w||})+b=0$$
$$\gamma_i=\frac{w^Tx_i+b}{||w||}.$$
Here we multiply the margin with $y_i$ so that it shows the correctness of prediction. 
$$\gamma_i=\frac{y_i(w^Tx_i+b)}{||w||}.$$
For the whole dataset, we define the geometric margin of the dataset as
$$\gamma=\min_{i=1,...,n}\frac{y_i(w^Tx_i+b)}{||w||}.$$

## The optimal margin classifier
The optimal margin classifier wants the margin defined above to be as large as possible, that is, 
$$\max_{w, b}\min_{i=1,...,n}\frac{y_i(w^Tx_i+b)}{||w||}$$
$$s.t.\quad y_i(w^Tx_i+b)>0\text{, }i=1,...,n.$$
This can be rewrite into
$$\max_{w, b}\frac{1}{||w||}\min_{i=1,...,n}y_i(w^Tx_i+b)$$
$$s.t.\quad y_i(w^Tx_i+b)>0\text{, }i=1,...,n.$$
We can find that $y_i(w^Tx_i+b)$ is the functional margin. We know that functional margin can be scaled without changing anything. Here we can add a constraint to the functional margin to a constant, here we take 1, meaning that
$$\hat{\gamma}=\min_{i=1,...,n}y_i(w^Tx_i+b)=1$$
Therefore, 
$$\max_{w, b}\frac{1}{||w||}$$
$$s.t.\quad y_i(w^Tx_i+b)\ge1,...,n,$$
that is, 
$$\min_{w, b}\frac{1}{2}w^Tw$$
$$s.t.\quad y_i(w^Tx_i+b)\ge1,...,n.$$
This becomes a convex optimization problem that can be solved by QP (quadratic programming). Solving this will give us the optimal margin classifier. 