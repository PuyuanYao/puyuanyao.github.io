---
title: Linear Regression (1)
date: 2023-8-12
categories: [regression]
tags: [linear regression]
math: true
---

For a given dataset, 

$$D=\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})\}\text{, }$$

we have

$$X=
\begin{pmatrix} 
    \text{ }—\text{ }(x^{(1)})^T—\text{ } \\
    \text{ }—\text{ }(x^{(2)})^T—\text{ } \\
    ... \\
    \text{ }—\text{ }(x^{(m)})^T—\text{ } \\
\end{pmatrix}_{m\times n},\quad
Y=
\begin{pmatrix}
    (y^{(1)})\\
    (y^{(2)})\\
    ...\\
    (y^{(m)})\\
\end{pmatrix}_{m\times 1},\quad$$

where each $x$ has $n$ dimensions, 

$$x^{(i)}=
\begin{pmatrix} 
    x^{(i)}_1\\
    x^{(i)}_2\\
    ...\\
    x^{(i)}_n\\
\end{pmatrix}_{n\times 1},i=1,2,...,m\quad.$$

In linear regression, we use a linear function to fit the data. For data that has $n$ dimensions, we use $n+1$ parameters, 

$$\theta=
\begin{pmatrix}
    b\\
    \theta_1\\
    \theta_2\\
    ...\\
    \theta_p\\
\end{pmatrix}_{n+1\times 1}.$$

In fact we can image there is an extra dimension of $x$, $x^{(n)}_0$ for $b$. So that we can simply write the linear function as

$$h_\theta(x^{(i)})=\theta^Tx^{(i)}=\sum^n_{j=1}\theta_j x^{(i)}_j.$$

To evaluate how well the function fits the data, we define the square error as the cost function, the Mean Square Error (MSE). 

$$J(\theta)=\frac{1}{2}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2.$$

The goal of linear regression is to find the $\theta$ that minimizes this cost function. To achieve this, we have several methods.

## Gradient decent
To start with, we need to have a initial guess of $\theta$, then update it repeatedly until it reaches minimal of cost function. 

$$\theta_j := \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta).$$

This an natural idea where we take steps in the direction of steepest decrease of the cost function. The learning rate $\alpha$ represent the size of each step.  

Derivation of cost function:

$$
\begin{aligned}
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial\theta_j}\frac{1}{2}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2\\
&=\sum^m_{i=1}\frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2\\
&=\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})\frac{\partial}{\partial\theta_j}(h_\theta(x^{(i)})-y^{(i)})\\
&=\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})\frac{\partial}{\partial\theta_j}\sum^n_{j=1}\theta_j x^{(i)}_j\\
&=\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
\end{aligned}$$

Therefore, 

$$\theta_j := \theta_j-\alpha\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j.$$

The method of gradient decent will update the parameter to the local optima that it subject to, but it does not guarantee a global optima. However, in this case where we are optimizing on cost function, it will always land on global optima because it's a quadratic function. Or, in other word, it will always converge.  

More specifically, the method above is called **batch gradient decent**, because it adds over all training examples' cost for every update on parameters. However, when we have large dataset, the action of adding over all training example could be slow and there won't be enough memory to store them.  

Therefore, instead of update parameters by summing over all training example's cost, we can update them for every single training example, where

$$\theta_j := \theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j.$$

which is called **stochastic gradient descent**.  

Both gradient decent methods are guaranteed to converge for this coast function. 

## The normal equations
Here we will use the vector representation of the dataset. In computer, matrix multiplication can speed up the process of computing and run on large set of data simultaneously. Also, this equation allows you to directly calculate the optimal parameters instead of updating them repetitively.  

Given

$$X=
\begin{pmatrix} 
    \text{ }—\text{ }(x^{(1)})^T—\text{ } \\
    \text{ }—\text{ }(x^{(2)})^T—\text{ } \\
    ... \\
    \text{ }—\text{ }(x^{(m)})^T—\text{ } \\
\end{pmatrix}_{m\times n},\quad
Y=
\begin{pmatrix}
    (y^{(1)})\\
    (y^{(2)})\\
    ...\\
    (y^{(m)})\\
\end{pmatrix}_{m\times 1},\quad
\theta=
\begin{pmatrix}
    \theta_1\\
    \theta_2\\
    ...\\
    \theta_p\\
\end{pmatrix}_{n\times 1},$$

where each $x$ has $n$ dimensions, 

$$x^{(i)}=
\begin{pmatrix} 
    x^{(i)}_1\\
    x^{(i)}_2\\
    ...\\
    x^{(i)}_n\\
\end{pmatrix}_{n\times 1},i=1,2,...,m\quad.$$

Now, we rewrite the cost function in matrix form,

$$\begin{aligned}
X\theta-Y&=
\begin{pmatrix}
(x^{(1)})^T\theta\\
(x^{(2)})^T\theta\\
...\\
(x^{(m)})^T\theta\\
\end{pmatrix}-
\begin{pmatrix}
y^{(1)}\\
y^{(2)}\\
...\\
y^{(m)}\\
\end{pmatrix}\\
&=
\begin{pmatrix}
h_\theta(x^{(1)})\\
h_\theta(x^{(2)})\\
...\\
h_\theta(x^{(m)})\\
\end{pmatrix},
\end{aligned}$$

$$\begin{aligned}
J(\theta)&=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)\\
&=\frac{1}{2}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2.
\end{aligned}$$



To minimize the cost function, we need to find its derivatives with respect to $\theta$,

$$\begin{aligned}
\nabla_\theta J(\theta)
&=\nabla_\theta\frac{1}{2}(X\theta-Y)^T(X\theta-Y)\\
&=\frac{1}{2}\nabla_\theta(\theta^TX^TX\theta-Y^TX\theta-\theta^TX^TY+Y^TY)\\
&=\frac{1}{2}\nabla_\theta(\theta^TX^TX\theta-2Y^TX\theta)\\
&=\frac{1}{2}(2X^TX\theta-2X^TY)\\
&=X^TX\theta-X^TY
\end{aligned}.$$


Solve for $\nabla_\theta J(\theta)=0$, we get

$$X^TX\theta=X^TY$$

$$\theta=(X^TX)^{-1}X^TY.$$  

